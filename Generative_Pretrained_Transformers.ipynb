{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d55163f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0055aa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dbff3e67",
   "metadata": {},
   "source": [
    "! pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74a90298",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFOpenAIGPTLMHeadModel, OpenAIGPTTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ad4556f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "881a6c3e0d87469dbab8c3aa12c76f4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.27M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n",
      "/Users/abhilashchauhan/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFOpenAIGPTLMHeadModel: ['h.7.attn.bias', 'h.0.attn.bias', 'h.5.attn.bias', 'h.4.attn.bias', 'h.3.attn.bias', 'h.6.attn.bias', 'h.10.attn.bias', 'h.8.attn.bias', 'h.2.attn.bias', 'h.1.attn.bias', 'h.9.attn.bias', 'h.11.attn.bias']\n",
      "- This IS expected if you are initializing TFOpenAIGPTLMHeadModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFOpenAIGPTLMHeadModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFOpenAIGPTLMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFOpenAIGPTLMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "gpttokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')\n",
    "gpt = TFOpenAIGPTLMHeadModel.from_pretrained('openai-gpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d441d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[5846 9259  544  481]], shape=(1, 4), dtype=int32)\n",
      "Output \n",
      " ----------------------------------------------------------------------------------------------------\n",
      "robotics is the only way to get to the surface. \" \n",
      " \" i'm not sure i understand. \" \n",
      " \" the first thing we have to do is find a way to get to the surface. \" \n",
      " \" but how? \" \n",
      " \" we have to find a way to get to the surface. \" \n",
      " \" but how? \" \n",
      " \" we have to find a way to get to the surface. \" \n",
      " \" but how? \" \n",
      " \" we have to find a way to\n"
     ]
    }
   ],
   "source": [
    "input_ids = gpttokenizer.encode('Robotics is the ', return_tensors='tf')\n",
    "print(input_ids)\n",
    "\n",
    "greedy_output = gpt.generate(input_ids, max_length=100)\n",
    "print(\"Output \\n \" + '-' * 100)\n",
    "print(gpttokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22dc6b87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "936b5a1c4afe40a1b13e1ab5c71bb938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhilashchauhan/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "gpt2tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "gpt2 = TFGPT2LMHeadModel.from_pretrained('gpt2', pad_token_id=gpt2tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb84cede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[14350 23891   318   262   220]], shape=(1, 5), dtype=int32)\n",
      "Output \n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Robotics is the vernacular of the future.\n",
      "\n",
      "The future is not a future where robots are going to be able to do anything. It's a future where robots are going to be able to do anything.\n",
      "\n",
      "The future is a future where robots are going to be able to do anything.\n",
      "\n",
      "The future is a future where robots are going to be able to do anything.\n",
      "\n",
      "The future is a future where robots are going to be able to do anything.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_ids = gpt2tokenizer.encode('Robotics is the ', return_tensors='tf')\n",
    "print(input_ids)\n",
    "\n",
    "greedy_output = gpt2.generate(input_ids, max_length=100)\n",
    "print(\"Output \\n \" + '-' * 100)\n",
    "print(gpt2tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7c286f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
